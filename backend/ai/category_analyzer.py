"""
Category Analyzer - AI-powered Naver category suggestion
Uses Gemini AI to analyze product data and suggest appropriate categories
"""
import os
import logging
import json
import re
from typing import List, Dict, Optional
import google.generativeai as genai

logger = logging.getLogger(__name__)


class CategoryAnalyzer:
    """
    AI-powered category analyzer using Gemini
    Suggests appropriate Naver SmartStore categories based on product data
    """

    def __init__(self):
        """Initialize Gemini AI client"""
        api_key = os.getenv('GEMINI_API_KEY')
        if not api_key:
            raise ValueError("GEMINI_API_KEY environment variable not set")

        genai.configure(api_key=api_key)
        self.model = genai.GenerativeModel('gemini-2.0-flash-exp')

        # Safety settings - relaxed for product analysis
        self.safety_settings = {
            'HARM_CATEGORY_HARASSMENT': 'BLOCK_NONE',
            'HARM_CATEGORY_HATE_SPEECH': 'BLOCK_NONE',
            'HARM_CATEGORY_SEXUALLY_EXPLICIT': 'BLOCK_NONE',
            'HARM_CATEGORY_DANGEROUS_CONTENT': 'BLOCK_NONE',
        }

        logger.info("âœ… CategoryAnalyzer initialized with Gemini 2.0 Flash")

    def flatten_categories(self, categories: List[Dict], parent_path: str = "") -> List[Dict]:
        """
        Flatten hierarchical category tree to leaf categories only

        Args:
            categories: Hierarchical category tree from Naver API
            parent_path: Path of parent categories (for breadcrumb)

        Returns:
            List of leaf categories with full paths
        """
        flat_categories = []

        for category in categories:
            category_name = category.get('name', category.get('categoryName', ''))
            category_id = category.get('id', category.get('categoryId', ''))

            # Build full path
            current_path = f"{parent_path} > {category_name}" if parent_path else category_name

            # Check if leaf category (no children or has wholeCategoryId which indicates leaf)
            children = category.get('children', category.get('childCategories', []))
            is_leaf = len(children) == 0 or 'wholeCategoryId' in category

            if is_leaf and category_id:
                flat_categories.append({
                    'id': str(category_id),
                    'name': category_name,
                    'path': current_path
                })

            # Recursively process children
            if children:
                flat_categories.extend(
                    self.flatten_categories(children, current_path)
                )

        return flat_categories

    def suggest_categories(
        self,
        product_data: Dict,
        categories_tree: List[Dict],
        top_k: int = 3
    ) -> List[Dict]:
        """
        Suggest appropriate Naver categories for a product using AI

        Args:
            product_data: {
                'title': Korean product title,
                'price': Product price,
                'desc': Optional product description,
                'images': Optional image URLs (list)
            }
            categories_tree: Full Naver category hierarchy
            top_k: Number of suggestions to return (default: 3)

        Returns:
            List of suggestions:
            [
                {
                    'category_id': '50000790',
                    'category_path': 'íŒ¨ì…˜ì˜ë¥˜/ì¡í™” > ì—¬ì„±ì‹ ë°œ > ìš´ë™í™”',
                    'confidence': 95,
                    'reason': 'ìƒí’ˆëª…ì— "ìš´ë™í™”" í‚¤ì›Œë“œ í¬í•¨'
                },
                ...
            ]
        """
        try:
            # Flatten categories to leaf nodes only
            leaf_categories = self.flatten_categories(categories_tree)

            if not leaf_categories:
                logger.error("âŒ No leaf categories found in category tree")
                return []

            logger.info(f"ğŸ“Š Analyzing product with {len(leaf_categories)} available categories")

            # Prepare product info
            title = product_data.get('title', '')
            price = product_data.get('price', 0)
            desc = product_data.get('desc', product_data.get('description', ''))
            images = product_data.get('images', [])

            if not title:
                logger.error("âŒ Product title is required for category analysis")
                return []

            # Build prompt
            prompt = self._build_analysis_prompt(title, price, desc, leaf_categories, top_k)

            # Prepare content for Gemini (with image if available)
            content_parts = [prompt]

            # Add first image for visual analysis (if available)
            if images and len(images) > 0:
                try:
                    import requests
                    from PIL import Image
                    import io

                    first_image_url = images[0]
                    logger.info(f"ğŸ–¼ï¸ Adding product image for visual analysis: {first_image_url}")

                    # Download image
                    response = requests.get(first_image_url, timeout=5)
                    if response.status_code == 200:
                        img = Image.open(io.BytesIO(response.content))
                        content_parts.append(img)
                        logger.info(f"âœ… Image added to analysis")
                    else:
                        logger.warning(f"âš ï¸ Failed to download image: {response.status_code}")
                except Exception as img_error:
                    logger.warning(f"âš ï¸ Image processing failed, continuing with text only: {str(img_error)}")

            # Call Gemini API
            logger.info(f"ğŸ¤– Calling Gemini AI for category analysis...")
            response = self.model.generate_content(
                content_parts,
                safety_settings=self.safety_settings
            )

            # Parse response
            suggestions = self._parse_ai_response(response.text, leaf_categories)

            logger.info(f"âœ… Generated {len(suggestions)} category suggestions")
            return suggestions[:top_k]

        except Exception as e:
            logger.error(f"âŒ Category analysis failed: {str(e)}", exc_info=True)
            return []

    def _extract_keywords(self, title: str) -> List[str]:
        """
        Extract relevant keywords from product title for category filtering

        Handles Korean compound nouns like:
        - ì‹¤ë‚´í™” (indoor shoes)
        - ìš´ë™í™” (sneakers)
        - ìŠ¬ë¦¬í¼ (slippers)

        Args:
            title: Product title in Korean

        Returns:
            List of keywords to match against categories
        """
        import re

        # Common Korean product category keywords (ì£¼ìš” ìƒí’ˆ ì¹´í…Œê³ ë¦¬ í‚¤ì›Œë“œ)
        category_keywords = [
            # ì‹ ë°œ
            'ìš´ë™í™”', 'ìŠ¬ë¦¬í¼', 'ì‹¤ë‚´í™”', 'ìƒŒë“¤', 'ë¶€ì¸ ', 'êµ¬ë‘', 'ë¡œí¼', 'ìŠ¤ë‹ˆì»¤ì¦ˆ', 'ì›Œì»¤',

            # ì˜ë¥˜
            'ì²­ë°”ì§€', 'ì›í”¼ìŠ¤', 'í‹°ì…”ì¸ ', 'í›„ë“œ', 'ë§¨íˆ¬ë§¨', 'ë°”ì§€', 'ì¹˜ë§ˆ', 'ì¬í‚·', 'ì½”íŠ¸', 'íŒ¨ë”©',
            'ë ˆê¹…ìŠ¤', 'ì¡°ê±°íŒ¬ì¸ ', 'ì¹´ë””ê±´', 'ë‹ˆíŠ¸', 'ë¸”ë¼ìš°ìŠ¤', 'ì…”ì¸ ', 'ì í¼',

            # íŒ¨ì…˜ì¡í™”
            'ê°€ë°©', 'ì§€ê°‘', 'ë²¨íŠ¸', 'ëª¨ì', 'ì¥ê°‘', 'ëª©ë„ë¦¬', 'ìŠ¤ì¹´í”„', 'ì–‘ë§', 'ìŠ¤íƒ€í‚¹',

            # í™”ì¥í’ˆ/ë¯¸ìš©
            'í™”ì¥í’ˆ', 'ë¡œì…˜', 'í¬ë¦¼', 'ì„¸ëŸ¼', 'ë§ˆìŠ¤í¬íŒ©', 'ë¦½ìŠ¤í‹±', 'íŒŒìš´ë°ì´ì…˜', 'ì„ í¬ë¦¼',
            'í´ë Œì§•', 'í† ë„ˆ', 'ì—ì„¼ìŠ¤', 'ì•„ì´í¬ë¦¼', 'íŒ©íŠ¸', 'ì¿ ì…˜', 'ì•„ì´ì„€ë„ìš°',

            # ì „ìì œí’ˆ
            'ì¼€ì´ìŠ¤', 'ì¶©ì „ê¸°', 'ì´ì–´í°', 'í—¤ë“œí°', 'í‚¤ë³´ë“œ', 'ë§ˆìš°ìŠ¤', 'ìŠ¤í”¼ì»¤', 'ë°°í„°ë¦¬',
            'ê±°ì¹˜ëŒ€', 'ë³´ì¡°ë°°í„°ë¦¬', 'ì¼€ì´ë¸”', 'ì–´ëŒ‘í„°',

            # ì¹¨êµ¬/ìƒí™œ
            'ë‹´ìš”', 'ì¿ ì…˜', 'ë°©ì„', 'ì´ë¶ˆ', 'ë² ê°œ', 'ì»¤íŠ¼', 'ëŸ¬ê·¸', 'ë§¤íŠ¸',

            # ê°€êµ¬
            'ì˜ì', 'ì±…ìƒ', 'ì¹¨ëŒ€', 'ìˆ˜ë‚©ì¥', 'ì„ ë°˜', 'í–‰ê±°', 'ì˜·ì¥', 'ì„œëì¥', 'ì†ŒíŒŒ',

            # ê³µêµ¬ (ì „ë™ê³µêµ¬)
            'ë“œë¦´', 'ë“œë¼ì´ë²„', 'ê·¸ë¼ì¸ë”', 'ìƒŒë”', 'ëŒ€íŒ¨', 'í†±', 'ë£¨í„°', 'ì„íŒ©íŠ¸',
            'ì „ë™ë“œë¦´', 'ì¶©ì „ë“œë¦´', 'ì „ë™ë“œë¼ì´ë²„', 'ì¶©ì „ë“œë¼ì´ë²„', 'ê°ë„ì ˆë‹¨ê¸°', 'ì›í˜•í†±',
            'ì „ê¸°í†±', 'ì»·ì˜', 'ë©€í‹°íˆ´', 'ê´‘íƒê¸°', 'ë¯¹ì„œê¸°', 'ì—ì–´ì»´í”„ë ˆì„œ',

            # ê³µêµ¬ (ìˆ˜ì‘ì—…ê³µêµ¬)
            'ë Œì¹˜', 'íœì¹˜', 'ë‹ˆí¼', 'ë§ì¹˜', 'ë“œë¼ì´ë²„', 'ëª½í‚¤', 'í†±', 'ì¤„', 'ëŒ',
            'ë°”ì´ìŠ¤', 'í´ë¨í”„', 'ì¹¼', 'ì»¤í„°', 'ê°€ìœ„', 'í€ì¹˜', 'ë¦¬ë²³', 'ì •',

            # ì¸¡ì •ê¸°
            'ì¤„ì', 'ìˆ˜í‰ê³„', 'ê°ë„ê¸°', 'ë ˆì´ì €ì¸¡ì •ê¸°', 'ì˜¨ë„ê³„', 'ìŠµë„ê³„',

            # ì›ì˜ˆ/ë†ì—…
            'ì˜ˆì´ˆê¸°', 'ë¶„ë¬´ê¸°', 'í˜¸ìŠ¤', 'ì‚½', 'ê´­ì´', 'ê°ˆí€´', 'ì „ì§€ê°€ìœ„', 'ì”ë””ê¹ì´',

            # ì•ˆì „ìš©í’ˆ
            'ì¥ê°‘', 'ë§ˆìŠ¤í¬', 'ê³ ê¸€', 'í—¬ë©§', 'ê·€ë§ˆê°œ', 'ì•ˆì „í™”', 'ì•ˆì „ëª¨',

            # ì£¼ë°©ìš©í’ˆ
            'ëƒ„ë¹„', 'í”„ë¼ì´íŒ¬', 'ì¹¼', 'ë„ë§ˆ', 'ë¯¹ì„œê¸°', 'ë¸”ë Œë”', 'ì—ì–´í”„ë¼ì´ì–´',

            # ì‹í’ˆ
            'ê³¼ì', 'ìŒë£Œ', 'ë¼ë©´', 'ì»¤í”¼', 'ì°¨', 'ìŒ€', 'ê°„ì‹', 'ê±´ê³¼ë¥˜',
        ]

        keywords = []

        # 1. Extract known category keywords from compound words
        for cat_keyword in category_keywords:
            if cat_keyword in title:
                keywords.append(cat_keyword)

        # 2. Remove common symbols and split
        title_clean = re.sub(r'[/\-_\[\](){}]', ' ', title)
        words = title_clean.split()

        # 3. Add individual words (ê¸¸ì´ >= 2, ìˆ«ì ì œì™¸)
        for word in words:
            word = word.strip()
            if len(word) >= 2 and not word.isdigit() and word not in keywords:
                keywords.append(word)

        # 4. Add original title for full context matching
        if title not in keywords:
            keywords.append(title)

        logger.info(f"ğŸ” Extracted keywords: {keywords[:10]}")
        return keywords

    def _build_analysis_prompt(
        self,
        title: str,
        price: float,
        desc: str,
        categories: List[Dict],
        top_k: int
    ) -> str:
        """Build structured prompt for Gemini AI"""

        # Smart filtering: extract keywords from product title
        keywords = self._extract_keywords(title)

        # Filter categories based on keywords for better relevance
        relevant_categories = []
        other_categories = []

        for cat in categories:
            # Check if any keyword matches category path
            is_relevant = any(keyword.lower() in cat['path'].lower() for keyword in keywords)

            if is_relevant:
                relevant_categories.append(cat)
            else:
                other_categories.append(cat)

        # Combine: prioritize relevant categories, then add others
        # Gemini 2.0 Flash has 1M token context, so we can include many categories
        selected_categories = relevant_categories[:500] + other_categories[:500]

        logger.info(f"ğŸ“Š Filtered {len(relevant_categories)} relevant categories (keywords: {keywords})")

        # Group by main category
        category_groups = {}
        for cat in selected_categories:
            parts = cat['path'].split(' > ')
            main_cat = parts[0] if parts else 'Unknown'

            if main_cat not in category_groups:
                category_groups[main_cat] = []

            category_groups[main_cat].append({
                'id': cat['id'],
                'path': cat['path']
            })

        # Format for prompt - include more categories for better accuracy
        categories_text = []
        for main_cat, subcats in category_groups.items():
            categories_text.append(f"\n[{main_cat}]")
            for subcat in subcats[:50]:  # Increased from 10 to 50 subcategories per main
                categories_text.append(f"  ID: {subcat['id']} - {subcat['path']}")

        categories_formatted = '\n'.join(categories_text)

        prompt = f"""ë‹¹ì‹ ì€ ë„¤ì´ë²„ ìŠ¤ë§ˆíŠ¸ìŠ¤í† ì–´ ìƒí’ˆ ì¹´í…Œê³ ë¦¬ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì£¼ì–´ì§„ ìƒí’ˆ ì •ë³´ë¥¼ **ê¹Šì´ ìˆê²Œ ë¶„ì„**í•˜ì—¬ ê°€ì¥ ì í•©í•œ ì¹´í…Œê³ ë¦¬ {top_k}ê°œë¥¼ ì¶”ì²œí•´ì£¼ì„¸ìš”.

**ìƒí’ˆ ì •ë³´:**
- ì œëª©: {title}
- ê°€ê²©: {price:,}ì›
- ì„¤ëª…: {desc[:200] if desc else 'ì—†ìŒ'}
{"- ì´ë¯¸ì§€: ì²¨ë¶€ë¨ (ìƒí’ˆ ì´ë¯¸ì§€ë¥¼ ìì„¸íˆ ê´€ì°°í•˜ì„¸ìš”)" if images else ""}

**ì‚¬ìš© ê°€ëŠ¥í•œ ì¹´í…Œê³ ë¦¬ (ì¼ë¶€):**
{categories_formatted}

**ë¶„ì„ ê°€ì´ë“œë¼ì¸ (ë§¤ìš° ì¤‘ìš”):**

1. **ì´ë¯¸ì§€ ìš°ì„  ë¶„ì„ (ì´ë¯¸ì§€ê°€ ìˆëŠ” ê²½ìš°)**
   - ì´ë¯¸ì§€ì—ì„œ ìƒí’ˆì˜ **ì‹¤ì œ í˜•íƒœ, êµ¬ì¡°, ë””ìì¸**ì„ ë¨¼ì € íŒŒì•…
   - ì œëª©ì˜ ë‹¨ì–´ë³´ë‹¤ **ì‹¤ì œ ë³´ì´ëŠ” í˜•íƒœ**ê°€ ë” ì¤‘ìš”
   - ì˜ˆ: ì œëª©ì— "ì‹¤ë‚´í™”"ë¼ê³  í•´ë„, ì´ë¯¸ì§€ê°€ ìŠ¬ë¦¬í¼ì²˜ëŸ¼ ìƒê²¼ìœ¼ë©´ â†’ ìŠ¬ë¦¬í¼ ì¹´í…Œê³ ë¦¬

2. **ìƒí’ˆì˜ ë³¸ì§ˆì„ íŒŒì•…í•˜ì„¸ìš”**
   - ì œëª©ì— ì—¬ëŸ¬ í‚¤ì›Œë“œê°€ ìˆì–´ë„, ìƒí’ˆì˜ **í•µì‹¬ ìš©ë„**ì™€ **ì‹¤ì œ í˜•íƒœ**ë¥¼ ìš°ì„  ê³ ë ¤
   - ì˜ˆ: "ë¦¬ë³¸ ì‹¤ë‚´í™”" â†’ ë¦¬ë³¸ì€ ì¥ì‹, ì‹¤ë‚´í™”ê°€ í•µì‹¬ì´ì§€ë§Œ **ìŠ¬ë¦¬í¼ í˜•íƒœ**ë¼ë©´ ìŠ¬ë¦¬í¼ ì¹´í…Œê³ ë¦¬ ìš°ì„ 

3. **í˜•íƒœì™€ ìš©ë„ë¥¼ í•¨ê»˜ ê³ ë ¤í•˜ì„¸ìš”**
   - "ì‹¤ë‚´í™”"ë¼ëŠ” ë‹¨ì–´ê°€ ìˆì–´ë„:
     * ìŠ¬ë¦¬í¼ í˜•íƒœ (ëˆ ì—†ì´ ì‹ ëŠ”, ë’¤ê¿ˆì¹˜ ê°œë°©) â†’ "ìŠ¬ë¦¬í¼" ì¹´í…Œê³ ë¦¬
     * ìš´ë™í™” í˜•íƒœ (ëˆìœ¼ë¡œ ë¬¶ëŠ”, ë°œëª©ê¹Œì§€ ê°ì‹¸ëŠ”) â†’ "ì‹¤ë‚´í™”" ë˜ëŠ” "ìš´ë™í™”" ì¹´í…Œê³ ë¦¬
     * ìƒŒë“¤ í˜•íƒœ â†’ "ìƒŒë“¤" ì¹´í…Œê³ ë¦¬

4. **ì¹´í…Œê³ ë¦¬ ìš°ì„ ìˆœìœ„**
   - ë” êµ¬ì²´ì ì´ê³  ì„¸ë°€í•œ ì¹´í…Œê³ ë¦¬ë¥¼ ìš°ì„  ì„ íƒ
   - ìƒìœ„ ì¹´í…Œê³ ë¦¬ë³´ë‹¤ í•˜ìœ„ ì¹´í…Œê³ ë¦¬ ì„ íƒ
   - ì˜ˆ: "ì‹ ë°œ" < "ìŠ¬ë¦¬í¼" < "ì‹¤ë‚´ ìŠ¬ë¦¬í¼"

5. **í‚¤ì›Œë“œ í•¨ì • ì£¼ì˜**
   - ì œëª©ì˜ ëª¨ë“  ë‹¨ì–´ë¥¼ ì¹´í…Œê³ ë¦¬ë¡œ ë°˜ì˜í•˜ì§€ ë§ ê²ƒ
   - ì¥ì‹ì  ìš”ì†Œ (ë¦¬ë³¸, í¬ê·¼, ê¸°ëª¨ ë“±)ëŠ” ì¹´í…Œê³ ë¦¬ê°€ ì•„ë‹ˆë¼ ìƒí’ˆ íŠ¹ì§•
   - í•µì‹¬ ìƒí’ˆ ìœ í˜•ì— ì§‘ì¤‘

6. **ì‹ ë¢°ë„ ì ìˆ˜ ê¸°ì¤€**
   - 95-100%: ì´ë¯¸ì§€ì™€ ì œëª©ì—ì„œ ìƒí’ˆ í˜•íƒœì™€ ìš©ë„ê°€ ëª…í™•í•˜ê²Œ ì¼ì¹˜
   - 80-94%: ìƒí’ˆ ìœ í˜•ì€ ë§ì§€ë§Œ ì„¸ë¶€ ì‚¬í•­ ë¶ˆí™•ì‹¤
   - 60-79%: ëŒ€ëµì ìœ¼ë¡œ ë§ì§€ë§Œ ë‹¤ë¥¸ í•´ì„ ê°€ëŠ¥
   - 60% ë¯¸ë§Œ: ì¶”ì²œí•˜ì§€ ë§ˆì„¸ìš”

**ì‘ë‹µ í˜•ì‹ (JSON only):**
```json
[
  {{
    "category_id": "ì¹´í…Œê³ ë¦¬ID",
    "category_path": "ì „ì²´ ì¹´í…Œê³ ë¦¬ ê²½ë¡œ",
    "confidence": 95,
    "reason": "ì´ë¯¸ì§€ ë¶„ì„ ê²°ê³¼: [ì‹¤ì œ í˜•íƒœ ì„¤ëª…] + ì¶”ì²œ ì´ìœ  (í•œêµ­ì–´, êµ¬ì²´ì ìœ¼ë¡œ)"
  }}
]
```

**ì¤‘ìš”**: JSONë§Œ ì¶œë ¥í•˜ì„¸ìš”. ë‹¤ë¥¸ ì„¤ëª…ì´ë‚˜ ë§ˆí¬ë‹¤ìš´ì€ ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”."""

        return prompt

    def _parse_ai_response(self, response_text: str, available_categories: List[Dict]) -> List[Dict]:
        """Parse Gemini AI response and validate category IDs"""

        try:
            # Extract JSON from response (may be wrapped in markdown code blocks)
            json_match = re.search(r'```(?:json)?\s*(\[.*?\])\s*```', response_text, re.DOTALL)
            if json_match:
                json_text = json_match.group(1)
            else:
                # Try to find JSON array directly
                json_match = re.search(r'\[.*?\]', response_text, re.DOTALL)
                if json_match:
                    json_text = json_match.group(0)
                else:
                    logger.error(f"âŒ No JSON found in AI response: {response_text[:200]}")
                    return []

            # Parse JSON
            suggestions = json.loads(json_text)

            if not isinstance(suggestions, list):
                logger.error(f"âŒ AI response is not a list: {type(suggestions)}")
                return []

            # Validate and enrich suggestions
            valid_suggestions = []
            category_map = {cat['id']: cat for cat in available_categories}

            for suggestion in suggestions:
                category_id = str(suggestion.get('category_id', ''))

                # Validate category ID exists
                if category_id not in category_map:
                    logger.warning(f"âš ï¸ Invalid category ID from AI: {category_id}")
                    continue

                # Enrich with actual category path from database
                valid_category = category_map[category_id]

                valid_suggestions.append({
                    'category_id': category_id,
                    'category_path': valid_category['path'],
                    'confidence': min(100, max(0, int(suggestion.get('confidence', 50)))),
                    'reason': suggestion.get('reason', 'ì¶”ì²œ ì¹´í…Œê³ ë¦¬')
                })

            return valid_suggestions

        except json.JSONDecodeError as e:
            logger.error(f"âŒ Failed to parse JSON from AI response: {str(e)}")
            logger.error(f"Response text: {response_text[:500]}")
            return []
        except Exception as e:
            logger.error(f"âŒ Failed to parse AI response: {str(e)}", exc_info=True)
            return []


# Singleton instance
_category_analyzer_instance = None


def get_category_analyzer() -> CategoryAnalyzer:
    """Get or create singleton CategoryAnalyzer instance"""
    global _category_analyzer_instance

    if _category_analyzer_instance is None:
        _category_analyzer_instance = CategoryAnalyzer()

    return _category_analyzer_instance
